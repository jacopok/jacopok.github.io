---
layout: post
title: Testing
date: 2022-11-14 17:39:00
description: 
tags: coding
# redirect: /assets/pdf/galileian_thesis.pdf
---

# Testing

We should make sure the code we write works.
That is fairly uncontroversial, but the way to practically test it is 
definitely nontrivial.

Often, in scientific code, what is tested are the end-to-end results of the code, but not the 
intermediate steps; these may only be tested informally, in an _ad hoc_ way, 
if at all.

In the `GWFish` case, end-to-end testing was performed by way of a comparison
with alternative software which did the same computations.
Specifically, cross-checks were performed between `GWFish`, `GWFast` [@iacovelliForecastingDetectionCapabilities2022]
and `GWBench` [@borhanianGwbenchNovelFisher2021] as an activity within the 
[Einstein Telescope Observational Science Board](https://www.et-gw.eu/index.php/observational-science-board),
which is an organization dedicated to developing the science case for the Einstein 
Telescope proposal. 
Since all these pieces of software are working with the same assumptions ---
Fisher matrix approximation, the same parametrization for the planned detectors,
_etc._ --- they should yield the same end result, and indeed, they did, 
at least for the situations considered.
This effort can give us confidence that those versions of all codes were working correctly
--- reaching the same, wrong result with three independent approaches is of course possible but unlikely (or it is an intrisic feature of the assumptions made by all three).

This kind of testing is definitely useful, but it is not the main subject of this section.
What we will discuss instead is how to make an _automated test suite_, which can be expanded
as more features are added to the code, and which may be made complete enough that the fact
it passes (i.e. runs without any errors) can give us a reasonable degree of confidence that our code is working sensibly.

We will not prove our code correct, but we can construct a series of checks that it is
not failing in any silly way. This simplifies the development process significantly, since
we can check at any time whether we have broken anything.
Also, it is not too difficult an extension to run our test suite in an isolated environment
with different software versions; this way we can make an informed claim about which 
ones our software supports and which it doesn't.

The golden standard in this regard is called _test driven development_, in which a workflow is adopted
in which a test is written _before_ the code which implements the feature it is testing.
Before getting to that, however, we shall discuss the simpler task of how to test existing code:
what paradigms and techniques we can use to construct good and convenient tests?

## Unit testing for matrix inversion

This section showcases how unit tests can be added to a relatively 
simple section of code: a function within `GWFish` meant to invert matrices, 
with some extra restrictions.
Really, while testing it we will see that it does not really compute the 
inverse of a matrix but a pseudoinverse; the tests here may be viewed 
as _exploratory_, I wrote them as I would when testing a "black box"
function whose behaviour in edge cases is unknown.

## Fisher matrix inversion and singularity issues

Within `GWFish`, an important step is the inversion of the Fisher matrix, which is 
required in order to provide estimates of the errors on the parameters.

This by itself does not seem like a difficult task computationally: 
after all, the matrices at hand are not very large (on the order of $$10\times 10$$). 
However, issues do arise due to the differences in the magnitude 
between the various components: this can be quantified through the
_conditioning number_, the ratio between the largest and smallest
eigenvalues.

The code which inverts the Fisher matrix within `GWFish` looks like this:

```python
import numpy as np

def invertSVD(matrix):
    thresh = 1e-10

    dm = np.sqrt(np.diag(matrix))
    normalizer = np.outer(dm, dm)
    matrix_norm = matrix / normalizer

    [U, S, Vh] = np.linalg.svd(matrix_norm)

    kVal = sum(S > thresh)
    matrix_inverse_norm = U[:, 0:kVal] @ np.diag(1. / S[0:kVal]) @ Vh[0:kVal, :]

    return matrix_inverse_norm / normalizer
```

How can we investigate whether this code will correctly invert 
a matrix?

## The simplest test

Let us start by building the simplest kind of test possible, which will already allow
us to showcase some ideas about automated testing.

We start by adding a testing function to the same script as the one in which the 
`invertSVD` function is defined.
The basic paradigm in testing is, of course, to run the code with some input 
and see whether it produces the correct result.

We will use a symmetric matrix, since all Fisher matrices are symmetric.
We will not check exact equality, since when working with floating 
point numbers that cannot be guaranteed.

```python
def test_matrix_inversion():

    matrix = np.array([[1, 3], [3, 4]])
    inverse = invertSVD(matrix)

    inverse_should_be = np.array([[-4/5, 3/5], [3/5, -1/5]])
    return np.allclose(inverse, inverse_should_be)

if __name__ == '__main__':
    print(test_matrix_inversion())
```

As expected, when running the script we get the result `True`;
on the other hand, if we change one of the numbers in the `inverse_should_be` 
matrix we get `False`.

Several problems with this appear right away: do we really need to manually 
compute matrix inverses to test our code?
We will get to that; first, though, let us _refactor_ our test.

As is, we need to actively look at the output of the script in order to 
see whether our test has succeeded or failed.
Also, the test and the actual code live in the same file, which is not 
great: as we add more tests, it will become a source of clutter.

## Using `pytest`

Our first refactoring step lies in moving the test code to its own script.
Also, as opposed to returning a boolean value, we will use an `assert` statement. 

`assert` is a convenient tool for debugging and testing: 
a statement like `assert x` will not do anything if `x` is truthy,[^truthy]
while it will fail with an `AssertionError` if `x` is falsey.

[^truthy]: "Truthy" in this context means that, when casted to a 
    boolean value, it will be cast to `True`. For example,
    `0` is falsey, while all other numbers are truthy.

So, our code will look like:

```python
from gwfish_matrix_inverse import invertSVD
import numpy as np

def test_matrix_inversion():

    matrix = np.array([[1, 3], [3, 4]])
    inverse = invertSVD(matrix)

    inverse_should_be = np.array([[-4/5, 3/5], [3/5, -1/5]])
    assert np.allclose(inverse, inverse_should_be)

if __name__ == '__main__':
    test_matrix_inversion()
```

and, unlike before, it will now not output anything if everything is working
correctly, and raise an error if not (try it!).

The next step is to try the same thing with the 
[`pytest` library](https://docs.pytest.org/en/7.2.x/).
We first need to install it (`pip install pytest`); after that, in 
the folder containing these files, we may simply run:

```bash
$ pytest
============================= test session starts ==============================
platform linux -- Python 3.9.11, pytest-7.1.3, pluggy-1.0.0
rootdir: /home/jacopo/Documents/clean-coding-thesis/scripts/testing_2
collected 1 item                                                               

test_matrix_inverse.py .                                                 [100%]

============================== 1 passed in 0.07s ===============================
```

`pytest` is able to go through the files in the folder, see that 
one of them has a name starting with `test_`, inside it 
there's a function starting with `test_`, run that function, find no 
errors, give us a success!

Note that now calling the function `test_matrix_inversion` in the script is not
required anymore: we may safely remove those last two lines.

What happens if the test fails? Here, `pytest` really shines: 
let us change one of the numbers in the should-be inverse, and re-run the same 
command:

```python
$ pytest
============================= test session starts ==============================
platform linux -- Python 3.9.11, pytest-7.1.3, pluggy-1.0.0
rootdir: /home/jacopo/Documents/clean-coding-thesis/scripts/testing_2
collected 1 item                                                               

test_matrix_inverse.py F                                                 [100%]

=================================== FAILURES ===================================
____________________________ test_matrix_inversion _____________________________

    def test_matrix_inversion():
    
        matrix = np.array([[1, 3], [3, 4]])
        inverse = invertSVD(matrix)
    
        inverse_should_be = np.array([[-4/5, 3/6], [3/5, -1/5]])
>       assert np.allclose(inverse, inverse_should_be)
E       assert False
E        +  where False = <function allclose at 0x7f25d5d27280>(array([[-0.8,  0.6],
       [ 0.6, -0.2]]), array([[-0.8,  0.5],\n       [ 0.6, -0.2]]))
E        +    where <function allclose at 0x7f25d5d27280> = np.allclose

test_matrix_inverse.py:10: AssertionError
=========================== short test summary info ============================
FAILED test_matrix_inverse.py::test_matrix_inversion - assert False
============================== 1 failed in 0.12s ===============================
```

`pytest` ran our test code and found an error.
It tells us exactly where it found it, and specifically how it came about:
we used the `np.allclose` function to check for the equality of two matrices 
which it prints out, so we can see at a glance what has gone wrong. 

Of course, we might not understand where the problem is right away in general,
but this all is about convenience, and having the tools at hand to spot 
as many details as possible.

### Debugging

Often, the information shown by `pytest` will still not be enough. 
A really convenient thing to be able to do, then, is to start from the 
failure and work interactively with the variables defined at that time:
this way, we can do all sorts of manipulations, or even make plots if we 
want!

This is easily achieved by adding the `--pdb` flag to our call to `pytest`.
`pdb`, "python debugger", is a standard tool for debugging, and it has
plenty of features worth exploring. Here I will give just a flavor of the 
possibilities. 

The shell looks like this:

```python
$ pytest --pdb
[... same output as before ...]
test_matrix_inverse.py:10: AssertionError
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

>>>>>>>>>>>>>>>>>> PDB post_mortem (IO-capturing turned off) >>>>>>>>>>>>>>>>>>>
> /home/jacopo/Documents/clean-coding-thesis/scripts/testing_2/
  test_matrix_inverse.py(10)test_matrix_inversion()
-> assert np.allclose(inverse, inverse_should_be)
(Pdb) matrix @ inverse
array([[ 1.00000000e+00, -1.11022302e-16],
       [ 4.44089210e-16,  1.00000000e+00]])
(Pdb) matrix @ inverse_should_be
array([[ 1.0000000e+00, -1.0000000e-01],
       [-4.4408921e-16,  7.0000000e-01]])
(Pdb) q


=========================== short test summary info ============================
FAILED test_matrix_inverse.py::test_matrix_inversion - assert False
!!!!!!!!!!!!!!!!!!! _pytest.outcomes.Exit: Quitting debugger !!!!!!!!!!!!!!!!!!!
```

It is hard to show in a fixed medium such as this, but after the test failure 
I was presented with a shell prompt `(Pdb)`, 
from which I could give arbitrary `python` commands.

I used it to compute the matrix product between the initial matrix
and the computed inverse (with the numpy shortcut `A@B`=$$AB$$ in a matricial sense), 
and the same with the manually-written inverse,
which showed that the computed inverse was indeed correct.

## Property-based testing

So far, we have tested the output of our code against a manually computed "correct result".
This is OK as far as it goes, but is necessarily only checks a few examples 
which we hope will be relevant, but which might not cover all edge cases.

In many situations, we may be able to find an _invariant_ in our code, which 
we expect to hold regardless of input. 
In this matrix inversion scenario, this is particularly simple: the defining property
of the inverse $$A^{-1}$$ of a matrix $$A$$ is that $$A^{-1} A = A A^{-1} = 1$$.

The first step in this direction is to refactor our test so that it can accept
any matrix: the following implementation uses the same matrix as before, 
but now we check the aforementioned property as opposed to the specific inverse.

```python
MATRIX = np.array([[1, 3], [3, 4]])

def test_matrix_inversion_constant_matrix(matrix = MATRIX):

    inverse = invertSVD(matrix)

    assert np.allclose(inverse@matrix, np.eye(*matrix.shape))
    assert np.allclose(matrix@inverse, np.eye(*matrix.shape))
```

The method `np.eye` is a convenient way to generate an identity
matrix with arbitrary shape.

This is the first step; what we could now do is to construct a 
method which generates random matrices and feed it to the algorithm.

This would already be quite good, but there is a better way, thanks to the 
[`hypothesis`](https://hypothesis.readthedocs.io/en/latest/index.html) library.

We are starting out on a fairly complex example (a matrix full of floating 
point numbers) for it --- if we were manipulating, say, strings, 
things could be quite a bit simpler ---; however, it is
the typical kind of task that we might need in a scientific 
context.

After installing `hypothesis` with the `numpy` extra (`pip install hypothesis[numpy]`),
we may use it as follows:

```python
from gwfish_matrix_inverse import invertSVD
import numpy as np
from hypothesis import given
from hypothesis import strategies as st
from hypothesis.extra.numpy import arrays

@given(arrays(np.float64, (2, 2)))
def test_matrix_inversion_hypothesis(matrix):
    
    inverse = invertSVD(matrix)

    assert np.allclose(inverse@matrix, np.eye(*matrix.shape))
    assert np.allclose(matrix@inverse, np.eye(*matrix.shape))
```

The `@given` decorator is what tells `hypothesis` to provide us with some
test data, of the kind specified in its argument: for us, numpy `arrays`. 
We then specify the data type (floating point numbers), the shape (which 
for now we keep as 2x2, we will generalize this later), and any extra conditions.
The data provided by `hypothesis` will not be _random_ but _arbitrary_:
it will purposefully try extreme examples, trying to get our code
to break. We will then be able to constrain the parameter space 
we allow it to explore when attempting this if we wish.

If we run this code, we get an immediate failure: I will not clutter 
this document with the full output, but the command to run is still `pytest`, 
which fails by raising an error in the SVD step with the falsifying example:

$$ A = \left[\begin{array}{cc}
0 & 0 \\ 
0 & 0
\end{array}\right]
$$

Fair enough: the inverse of the zero matrix does not exist.
One would now probably think of somehow restricting the examples
to invertible matrices, but let us try to simply put a lower
bound on the numbers allowed, so that they cannot be zero:

```python
from gwfish_matrix_inverse import invertSVD
import numpy as np
from hypothesis import given
from hypothesis import strategies as st
from hypothesis.extra.numpy import arrays

@given(arrays(np.float64, (2, 2), elements=st.floats(min_value=1e-20)))
def test_matrix_inversion_hypothesis(matrix):
    
    inverse = invertSVD(matrix)

    assert np.allclose(inverse@matrix, np.eye(*matrix.shape))
    assert np.allclose(matrix@inverse, np.eye(*matrix.shape))
```

As expected, the code fails with a singular matrix,

$$ A = \left[\begin{array}{cc}
1 & 1 \\ 
1 & 1
\end{array}\right]
$$

but surprisingly it does not raise an error: instead,
it returns the matrix

$$ A^{-1} \overset{?}{=} \left[\begin{array}{cc}
0.25 & 0.25 \\ 
0.25 & 0.25
\end{array}\right].
$$

The test then fails on the step of checking that this 
is the inverse (since it is not).

This is getting to the problem which really does occur 
in these computations: the Fisher matrix $$\mathcal{F}$$ is often 
singular or nearly-singular, and in order to deal with this
the quantity this code is computing is actually not the matrix inverse, 
but the _Moore-Penrose pseudoinverse_,
which is only the correct inverse in the subspace defined by the span of the matrix
(to numerical precision, that is, with very small eigenvalues being approximated as zero).
Formally, instead of $$A A^{-1} = A^{-1} A = 1$$, this pseudoinverse $$A^+$$ must satisfy
$$A^+ A A^+ = A^+$$ and $$A A^+ A = A$$:
let us therefore check these conditions.

We really should test this only the kinds of inputs we expect to be possible.
Doing so in this case turned out to be tricky but possible with `hypothesis`.

Gravitational-wave Fisher matrices can always be written as $$M_{ij} = \vec{v}_i \cdot \vec{v}_j$$
for vectors $$\vec{v}$$ lying in some high-dimensional vector space (specifically, the
vector space is the Hilbert space of waveforms with the product $$(\cdot | \cdot)$$, and the 
vectors are the derivatives $$\vec{v}_i = \partial _i h$$).
They can therefore be expressed as $$M_{ij} = |v_i| |v_j| \cos(\theta _{ij})$$.
This is definitely not true for all matrices!

The following test generates arbitrary vectors $$v_i$$ and cosines $$c_{ij}\in [-1,1]$$,
and then the matrices as $$M_{ij} = v_i v_j c_{ij}$$.
The set of matrices that can be generated this way is a superset of
the one of valid Fisher matrices.

The conditions $$c_{ii} = 1$$ and $$c_{ij} = c_{ji}$$, which will always hold for
angles amongst vectors, are enforced _a posteriori_.

```python
from gwfish_matrix_inverse import invertSVD
import numpy as np
from hypothesis import given, reject, target, seed
from hypothesis import strategies as st
from hypothesis.extra.numpy import arrays
import pytest

MATRIX_DIMENSION = 4
ABS_TOLERANCE = 1e-1
REL_TOLERANCE = 1e-2
MIN_NORM = 1e-5
MAX_NORM = 1e5


@seed(1)
@given(
    vector_norms=arrays(
        np.float64,
        (MATRIX_DIMENSION,),
        elements=st.floats(
            min_value=MIN_NORM,
            max_value=MAX_NORM,
        ),
        unique=True,
    ),
    cosines=arrays(
        np.float64,
        (MATRIX_DIMENSION, MATRIX_DIMENSION),
        elements=st.floats(
            min_value=-1.0,
            max_value=1.0,
        ),
        unique=True,
    ),
)
def test_matrix_pseudoinverse_hypothesis(vector_norms, cosines):

    cosines[np.arange(MATRIX_DIMENSION), np.arange(MATRIX_DIMENSION)] = 1
    cosines = np.maximum(cosines, cosines.T)

    matrix = np.outer(vector_norms, vector_norms) * cosines

    inverse = invertSVD(matrix)

    assert np.allclose(
        inverse @ matrix @ inverse, inverse, atol=ABS_TOLERANCE, rtol=REL_TOLERANCE
    )
    assert np.allclose(
        matrix @ inverse @ matrix, matrix, atol=ABS_TOLERANCE, rtol=REL_TOLERANCE
    )
```

This is not the be-all-and-end all for this kind of test: for one,
the tolerances were hand-picked, and a better understanding of the 
numerical problem is desirable.
Still, this showcases how powerful this property-based testing framework is.
Not all tests can be formally specified in this way, 
but many can, and if possible writing a test of this sort is very
powerful.

## Testing against different versions with `tox`

Ideally, we would like to not rely on a specific version of our dependencies,
but instead to support a range of versions for them.
Testing every possible combination is unfeasible,
but we can go to some length by at least checking every version 
of our "main" dependencies.
For example, we can check that our software is installable
with every currently supported version of `python`. 
This section will discuss a tool to automate this: 
[`tox`](https://tox.wiki/en/latest/). 

It allows us to run our tests in a freshly created 
virtual environment with arbitrary package versions. 
In order to use it we need to structure our code snippet as a package,
so we will use `poetry` with a `pyproject.toml` file as follows:

```toml
[tool.poetry]
name = "matrix_inverse"
version = "0.1.0"
description = ""
authors = ["jacopo <jacopo.tissino@gssi.it>"]
packages = [{include = "matrix_inverse"}]

[tool.poetry.dependencies]
python = ">=3.8, <3.12"
pytest = "^7.2.0"
hypothesis = "^6.56.4"
numpy = "^1.23.4"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

Then, we need to move our code to a folder called `matrix_inverse`.
Further, for consistency we will move the test code in a folder
called `tests`.
Finally, we can install `tox` and create a `tox.ini` file containing

```ini
[tox]
skipsdist = true
envlist = py{38,39,310,311}
isolated_build = true

[testenv]
deps = 
    poetry
commands =
    poetry install
    poetry run pytest {posargs}
```

Then we are done! We can run `tox` from the shell, which will create 
virtual environments with our dependencies for python 3.8, 3.9, 3.10 
and 3.11; it will then run the tests within these.

This takes a while the first time and the output is very long, but it ends with 

```bash
__________________________________ summary ___________________________________
  py38: commands succeeded
  py39: commands succeeded
  py310: commands succeeded
  py311: commands succeeded
  congratulations :)
```

We can check that the system is working properly by trying to exploit
some functionality that is available only in certain python releases.
For example, the package `tomllib` was added to the standard python 
library in version 3.11; before, one would have had to import it manually.
So, if without further changes we add a line `import tomllib` 
to our module or test code, we expect to get an error for versions 
3.10 and below; indeed, the output is

```bash
__________________________________ summary ___________________________________
ERROR:   py38: commands failed
ERROR:   py39: commands failed
ERROR:   py310: commands failed
  py311: commands succeeded
```

Running such a combination of tests can get expensive as the
test suite grows, but it may be worth it, especially if we require backward
compatibility.
For example, I often use the latest `python` release, but sometimes develop 
code that will be run on clusters whose installations are only updated 
irregularly; it is therefore important for me to be able to easily check that 
the code I am writing is compatible with the version installed there.

## When to write tests

Writing tests is time-consuming, 
but once they have been written they are extremely useful. 
When is a good time to write them? 

If we are developing a new feature, a good answer to this is: _before_ writing the 
code that implements the feature. 
This practice is known as __test-driven development__ (TDD), and it is more than twenty years
old. The idea is that writing the test first forces the developer to think about 
what the thing they are implementing should accomplish.

If this seems hard, that's a feature, not a bug: writing code without being able to 
formulate a test case for it is indicative of the code being not well-thought-out, 
or perhaps not modular enough.

This may be applied to new code, but often the real situation is that we have a large module
without any tests: what to do then?
One approach, which will be showcased in the next chapter, is to start by creating some 
__end-to-end__ tests to document the expected behavior of the code, and then refactor / simplify it as needed (since it will probably need refactoring).

Another opportunity to write tests is in the event of __bugs__ or problems: if we encounter 
a failure for a certain input to our code, we can calcify that input as a test case.
In `pytest`, the `@pytest.mark.xfail` decorator can help in clarifying: the
test we just wrote is known to be currently failing.
When the bug is fixed, the test should succeed, and it can be kept in our suite, allowing 
us to avoid regressing on that bug fix.

Finally, the kind of test development shown in the previous sections is a sort of 
"__exploratory testing__", which is yet another context in which tests can be written.
If we have some functionality which we do not completely understand the behavior of,
perhaps because it was written by somebody else, or maybe it even lies in a different
package. 
As we are trying to make sense of it --- as we would need to anyway --- we can write tests
cementing our understanding of its behavior.
Besides allowing us to clarify our understanding in this exploratory phase, 
the tests will remain there acting as a safeguard against changes in the functionality,
in the case that it is externally managed. 
For example, if we are relying on an external library which gets an update, 
we can quickly check whether it breaks our use case.